{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads a custom module I made for processing web of science data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import biblionet as bn\n",
    "import numpy as np\n",
    "import json\n",
    "import collections as c\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process Raw Web of Science Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This processes the data and the metadata, and it does standard data cleaning. You can skip this section and load the pre-processed data in the next section, if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point to where you have stored the data\n",
    "# The data is tab separated WOS files, stored utf-8, with 500 records per file\n",
    "path = \"C:/Users/LocalAdmin/Operations/Programming/data/TFSC\"\n",
    "# load the data\n",
    "corpus = bn.create_corpus(path)\n",
    "# create the metadata \n",
    "metadata = bn.create_metadata(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate the corpus with cleaned and processed text\n",
    "# this creates new fields (F1, F2, F3)\n",
    "cites = bn.annotate(\"F1\",corpus)\n",
    "orgs = bn.annotate(\"F2\",cites)\n",
    "content = bn.annotate(\"F3\",orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the top n words for indexing out of the cleaned content field\n",
    "content_dict = bn.make_index(1000,\"F3\",content)\n",
    "# index the data\n",
    "index = bn.index_corpus(\"F3\",content_dict,content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many articles you have indexed\n",
    "n=len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TFSCcontent.json', 'w') as fp:\n",
    "    json.dump(index, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now actually create the matrix. This is necessary for statistical processing. The data in the index is a dictionary and not in an appropriate format for statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from file if you skipped the previous section\n",
    "# point to the data if not local to the session\n",
    "with open('TFSCcontent.json', 'r') as fp:\n",
    "    index = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension the matrix\n",
    "# this stores a count vector for each word in the index in each row\n",
    "# this is repeated for all n articles\n",
    "X = np.zeros((n,1000))\n",
    "\n",
    "# now fill the count matrix\n",
    "i = 0\n",
    "for key in index:\n",
    "    array = index[key]\n",
    "    for el in array:\n",
    "           X[i,el]=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we include all index records, regardless of the amount of indexed content. This means that longer records receive more detail in the modelling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the matrix decomposition\n",
    "# This is equivalent to topic modelling, but a little better behaved on correlated data\n",
    "model = NMF(n_components=9, init='random', random_state=0)\n",
    "\n",
    "# W is an assignment vector, assigning each article to a mix of nine components\n",
    "W = model.fit_transform(X)\n",
    "# H is a topic model, assigning each index word to one or more of the nine topics\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process the Data to Create Citation Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step uses the pre-processed Web of Science data (index), as well as the assignment matrix of the topic model (W). The objective is to count all the citations in one of the core topics in TFSC. The topics are in no specific order, but I know from previous explorations using word clouds that we are interested in topic 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a place to store the counted citations\n",
    "cntr = c.Counter()\n",
    "\n",
    "# go through the articles one by one\n",
    "for n,key in enumerate(index):\n",
    "    arec = content[key]\n",
    "    # get the citation field from each record\n",
    "    cites = arec[\"CR\"]\n",
    "    # find the associated weight for each topic in the assignment matrix\n",
    "    w = W[n,6]\n",
    "    cite_list = cites.split(\"; \")\n",
    "    # iterate through each citation from the article\n",
    "    for cite in cite_list:\n",
    "        # store it with the appropriate weight.\n",
    "        cntr[cite]+=w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DOSI G, 1982, RES POLICY, V11, P147, DOI 10.1016/0048-7333(82)90016-6',\n",
       "  14.750214329259402),\n",
       " ('COHEN WM, 1990, ADMIN SCI QUART, V35, P128, DOI 10.2307/2393553',\n",
       "  13.169027689312447),\n",
       " ('Daim TU, 2006, TECHNOL FORECAST SOC, V73, P981, DOI 10.1016/j.techfore.2006.04.004',\n",
       "  13.048041938010158),\n",
       " ('Nelson R. R., 1982, EVOLUTIONARY THEORY', 12.857677458886538),\n",
       " ('EISENHARDT KM, 1989, ACAD MANAGE REV, V14, P532, DOI 10.2307/258557',\n",
       "  12.13388166635403),\n",
       " ('Phaal R, 2004, TECHNOL FORECAST SOC, V71, P5, DOI 10.1016/S0040-1625(03)00072-6',\n",
       "  11.730243872873515),\n",
       " ('BASS FM, 1969, MANAGE SCI, V15, P215, DOI 10.1287/mnsc.15.5.215',\n",
       "  11.36911269184459),\n",
       " ('Kostoff RN, 2001, IEEE T ENG MANAGE, V48, P132, DOI 10.1109/17.922473',\n",
       "  10.346945848294201),\n",
       " ('Hekkert MP, 2007, TECHNOL FORECAST SOC, V74, P413, DOI 10.1016/j.techfore.2006.03.002',\n",
       "  8.902654613632873),\n",
       " ('Geels FW, 2002, RES POLICY, V31, P1257, DOI 10.1016/S0048-7333(02)00062-8',\n",
       "  8.585792751771542),\n",
       " ('ANDERSON P, 1990, ADMIN SCI QUART, V35, P604, DOI 10.2307/2393511',\n",
       "  7.886181040592615),\n",
       " ('Bergek A, 2008, RES POLICY, V37, P407, DOI 10.1016/j.respol.2007.12.003',\n",
       "  7.1491792680187105),\n",
       " ('', 6.962027552797993),\n",
       " ('TEECE DJ, 1986, RES POLICY, V15, P285, DOI 10.1016/0048-7333(86)90027-2',\n",
       "  6.943374521198633),\n",
       " ('TUSHMAN ML, 1986, ADMIN SCI QUART, V31, P439, DOI 10.2307/2392832',\n",
       "  6.852949440990748),\n",
       " ('DAVIS FD, 1989, MIS QUART, V13, P319, DOI 10.2307/249008',\n",
       "  6.723306624983025),\n",
       " ('HENDERSON RM, 1990, ADMIN SCI QUART, V35, P9, DOI 10.2307/2393549',\n",
       "  6.606319536308636),\n",
       " ('FISHER JC, 1971, TECHNOL FORECAST SOC, V3, P75, DOI 10.1016/S0040-1625(71)80005-7',\n",
       "  6.518215943618239),\n",
       " ('Rogers E. M., 2003, DIFFUSION INNOVATION', 6.511902736574997),\n",
       " ('Walsh ST, 2004, TECHNOL FORECAST SOC, V71, P161, DOI 10.1016/j.techfore.2003.10.003',\n",
       "  6.400979958155172),\n",
       " ('Lee S, 2009, TECHNOL FORECAST SOC, V76, P769, DOI 10.1016/j.techfore.2009.01.003',\n",
       "  6.258492254626971),\n",
       " ('Watts RJ, 1997, TECHNOL FORECAST SOC, V56, P25, DOI 10.1016/S0040-1625(97)00050-4',\n",
       "  6.244382187670753),\n",
       " ('GRILICHES Z, 1990, J ECON LIT, V28, P1661', 6.231473795408928),\n",
       " ('Kostoff RN, 2004, TECHNOL FORECAST SOC, V71, P141, DOI 10.1016/S0040-1625(03)00048-9',\n",
       "  6.200088940711795),\n",
       " ('COHEN WM, 1989, ECON J, V99, P569, DOI 10.2307/2233763', 6.101666207183554),\n",
       " ('Etzkowitz H, 2000, RES POLICY, V29, P109, DOI 10.1016/S0048-7333(99)00055-4',\n",
       "  5.953135764338507)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the most common entries\n",
    "cntr.most_common(26)\n",
    "# We need to include one extra element to get 25 records. \n",
    "# This is because one of the records is a blank field; this escaped data cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
